
import json
from modulefinder import test
from pathlib import Path
from typing import List
from smolagents import CodeAgent
from models import create_azure_model, load_keys
from dataset import DataSet
from system_prompts import prompts
from judge_agent import EvaluationResults, build_judge_prompt_csv, build_judge_prompt_giab, parse_agent_outputs, parse_agent_results, eval_giab_metrics

# Create output directories
def create_dirs(prefix: str):
    outputs_path = Path(prefix) / "outputs"
    results_path = Path(prefix) / "results"
    outputs_path.mkdir(parents=True, exist_ok=True)
    results_path.mkdir(parents=True, exist_ok=True)

def glob_input_data(data_dir: Path, ref_dir: Path):
    return [
        *[p for p in data_dir.rglob("*") if p.is_file()],
        *[p for p in ref_dir.rglob("*") if p.is_file()],
    ]

datasets = DataSet.load_all()


for task in datasets:
    if task.task_id != 'alzheimer-mouse':
        continue
    print(f"Processing task: {task.task_id} at {task.path}")
    print(task.task_id)

    test_path = Path('test_outputs/' + task.task_id)
    # create_dirs(test_path)

    agent = CodeAgent(
            max_steps=10,
            model=create_azure_model(),
            tools=[],
            additional_authorized_imports=["*"],
        )
    agent.prompt_templates['system_prompt'] = prompts['v1']
    agent.run(task.task_prompt)

    """Block for collecting data for judge LLM"""
    # Files that were provided to the agent
    input_data = glob_input_data(
        test_path / "data",
        test_path / "reference"
    )
    # This parses the filepaths that have been generated by the agent
    agent_output_tree = parse_agent_outputs(test_path)

    # This parses the final results generated by the agent
    if task.task_id == "giab":
        agent_results = eval_giab_metrics(
            test_path / "results",
            test_path / "results", 
            test_path / "data" / "Agilent_v7.chr.bed",
            test_path / "reference" / "Homo_sapiens_assembly38.fasta",
        )
        agent_prompt = build_judge_prompt_giab(
            input_data,
            task.task_prompt,
            agent_output_tree,
            agent_results,
        )
        print(agent_prompt)
    
    else:
        agent_results = parse_agent_results(test_path / "results")
        truth_results = parse_agent_results(test_path / "results")
        agent_prompt = build_judge_prompt_csv(
            input_data,
            task.task_prompt,
            agent_output_tree,
            agent_results,
            truth_results,
        )

    # Evaluate the results using an LLM
    client = create_azure_model(framework='openai')
    response = client.chat.completions.create(
        model="gpt-5-mini",
        messages=[{"role": "user", "content": agent_prompt}],
    ).choices[0].message.content
    final_result = EvaluationResults(**json.loads(response))


        




